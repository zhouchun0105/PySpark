# collaborative filtering - alternating least square
from pyspark.mllib.recommendation import ALS

# classification
from pyspark.mllib.classification import LogisticRegressionWithLBFGS

# clustering
from pyspark.mllib.clustering import KMeans

# Rating class
from pyspark.millib.recommendation import Rating
r = Rating(user=1,product=2, rating=5.0)
(r[0],r[1],r[2])

# Splitting the data
data = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
training, test = data.randomSplit([0.6,0.4])
training.collect()
test.collect()

# rank-number of features, iterations-number of iterations to run the least squares computation
r1 = Rating(1,1,1.0)
r2 = Rating(1,2,2.0)
r3 = Rating(2,1,2.0)
ratings.sc.parallelize([r1,r2,r3])
ratings.collect()
model = ALS.train(ratings, rank=10,iterations=10)

unrated_RDD = sc.parallelize([(1,2),(1,1)])
predictions = model.predictAll(unrated_RDD)
predictions.collect()

# Model Evaluation using MSE (actual rating - predicted rating)
rates = rating.map(lambda x: ((x[0],x[1]),x[2]))
rates.collect()
preds = predictions.map(lambda x: ((x[0] ,x[1]),x[2]))
preds.collect()
rates_preds = rates.join(preds)
rates_preds.collect()
MSE = rates_preds.map(lambda r:(r[1][0]-r[1][1])**2).mean()

# Preprocessing example
# Load the data into RDD
data = sc.textFile('/usr/local/share/datasets/ratings.csv')

# Split the RDD 
ratings = data.map(lambda l: l.split(','))

# Transform the ratings RDD 
ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))

# Split the data into training and test
training_data, test_data = ratings_final.randomSplit([0.8, 0.2])

# Continue with model training and predictions datacamp






